# DPDK TX Path: Multi-Layer Architecture Overview

> **Document Purpose**: Understanding DPDK's layered architecture, data structures, and packet I/O flow
> **Project**: DPDK Performance Benchmarking & Optimization

---

## Table of Contents

1. [High-Level Architecture Overview](#1-high-level-architecture-overview)
2. [Complete Layer Stack Diagram (Detailed)](#2-complete-layer-stack-diagram-detailed)
3. [Data Structure Ownership & Lifecycle](#3-data-structure-ownership--lifecycle)
4. [TX Path Flow with Data Structures](#4-tx-path-flow-with-data-structures)
5. [Key Indices & Queue Depth Calculation](#5-key-indices--queue-depth-calculation)
6. [Our Tracking Points](#6-our-tracking-points)
7. [Configuration Bug Fixed](#7-configuration-bug-fixed)
8. [Summary Table](#8-summary-table)

---

## 1. High-Level Architecture Overview

### DPDK 4-Layer Architecture

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ  APPLICATION LAYER                                                  â”ƒ
â”ƒ  â€¢ Packet generation logic (Pktgen)                                 â”ƒ
â”ƒ  â€¢ Business logic, TX/RX orchestration                              â”ƒ
â”ƒ  â€¢ Temporary working buffers (pointer arrays)                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â†“ Uses APIs
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ  DPDK CORE LIBRARIES LAYER (librte_* - Run-Time Environment)        â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”ƒ
â”ƒ  â”‚  MEMPOOL     â”‚  MBUF        â”‚  ETHDEV                      â”‚    â”ƒ
â”ƒ  â”‚  (Container) â”‚  (Object)    â”‚  (API Abstraction)           â”‚    â”ƒ
â”ƒ  â”‚              â”‚              â”‚                              â”‚    â”ƒ
â”ƒ  â”‚  Memory pool â”‚  Packet      â”‚  Unified API for all NICs   â”‚    â”ƒ
â”ƒ  â”‚  management  â”‚  buffer:     â”‚  â€¢ rte_eth_tx_burst()        â”‚    â”ƒ
â”ƒ  â”‚              â”‚  metadata +  â”‚  â€¢ rte_eth_rx_burst()        â”‚    â”ƒ
â”ƒ  â”‚              â”‚  data        â”‚                              â”‚    â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â†“ Dispatches to PMD (via function pointers)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ  PMD LAYER (Poll Mode Driver)                                       â”ƒ
â”ƒ  â€¢ HW-specific driver (MLX5, i40e, etc.)                            â”ƒ
â”ƒ  â€¢ WQE (HW descriptors) ring buffer management                      â”ƒ
â”ƒ  â€¢ Completion queue polling                                         â”ƒ
â”ƒ  â€¢ DMA setup and doorbell operations                                â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â†“ DMA operations
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ  HARDWARE LAYER (NIC)                                               â”ƒ
â”ƒ  â€¢ Physical network interface card                                  â”ƒ
â”ƒ  â€¢ DMA engine, on-chip queues                                       â”ƒ
â”ƒ  â€¢ Wire transmission/reception                                      â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â†“
                      [ Network Wire ]
```

### Key Concepts

**Layer Separation**:
- **Application**: Business logic, uses DPDK APIs
- **DPDK Core Libraries**: Reusable infrastructure (mempool, mbuf, ethdev)
- **PMD**: Hardware-specific drivers (Intel, Mellanox, etc.)
- **Hardware**: Physical NIC

**Data Ownership**:
- **Application**: application buffer holding mbuf pointers (pinfo->tx_pkts[], DEFAULT_PKT_TX_BURST=32, MAX_PKT_TX_BURST 256)
- **DPDK Core**: Owns mbuf memory (mempool), dispatches API calls to the specific HW PMD
- **PMD**: Owns WQE Ring, Completion Queue, elts[] array (all in host memory)
- **Hardware**: Physical transmission

**API Flow** (TX path):
```
Application:  rte_mempool_get_bulk()         â†’  get mbufs
              (pktgen.c:1323 â†’ rte_mempool.h:1691)

              rte_eth_tx_burst()             â†’  submit packets
              (pktgen.c:569 â†’ rte_ethdev.h)

DPDK Core:    dev->tx_pkt_burst()            â†’  dispatch to PMD via function pointer
              (inside rte_eth_tx_burst, rte_ethdev.h)

PMD:          mlx5_tx_burst()                â†’  write WQEs, ring doorbell
              (mlx5_tx.h)

              mlx5_tx_handle_completion()    â†’  poll CQ, find completed WQEs
              (mlx5_tx.c:312)
                mlx5_tx_free_elts()          â†’  retrieve mbufs from elts[]
                (mlx5_tx.h:671, called at mlx5_tx.c:289)
                  mlx5_tx_free_mbuf()        â†’  group mbufs by mempool
                  (mlx5_tx.h:542, called at mlx5_tx.h:700)
                    rte_mempool_put_bulk()   â†’  return mbufs to mempool (free)
                    (rte_mempool.h:1474, called at mlx5_tx.h:566,621)
```

---

## 2. Complete Layer Stack Diagram (Detailed)

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                                      APPLICATION LAYER (Pktgen)                                                â”ƒ
â”ƒ                                                                                                                â”ƒ
â”ƒ  ğŸ“ Code Location: Pktgen-DPDK/app/pktgen.c                                                                   â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ  â”‚  ğŸ”§ TX Logic Functions & API Calls:                                                                    â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  [INITIALIZATION - Once at startup]                                                                    â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen_packet_ctor() (pktgen.c:750) - Construct packet templates                                   â”‚   â”ƒ
â”ƒ  â”‚    Creates pre-filled packets with headers (Ethernet, IP, TCP) and payload                            â”‚   â”ƒ
â”ƒ  â”‚    âš ï¸  Packet template reuse: pre-filled packets reused every burst                                   â”‚   â”ƒ
â”ƒ  â”‚    âš ï¸  NOT real TCP, just benchmark tool (e.g., sequence numbers are not correct)                     â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen_tx_workq_setup() (pktgen.c:1475) - Register TX functions to work queue                      â”‚   â”ƒ
â”ƒ  â”‚    API: workq_add(WORKQ_TX, pid, pktgen_main_transmit) (pktgen.c:1488)                               â”‚   â”ƒ
â”ƒ  â”‚    Registers pktgen_main_transmit() for repeated execution in main loop                                 â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  [FAST PATH TX - Main transmit loop]                                                                   â”‚   â”ƒ
â”ƒ  â”‚  Per-core main loop (pktgen_main_rxtx_loop at pktgen.c:1556):                                        â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Each lcore runs independent loop: lid = rte_lcore_id() (pktgen.c:1560)                            â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Each lcore handles its own qid: rx_qid = l2p_get_rxqid(lid) (pktgen.c:1574)                       â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  Main loop (pktgen.c:1585) repeatedly calls:                                                          â”‚   â”ƒ
â”ƒ  â”‚  â€¢ workq_run(WORKQ_TX, pid, qid) - Execute all registered TX functions                               â”‚   â”ƒ
â”ƒ  â”‚    Work queue mechanism: functions registered once, executed every iteration                          â”‚   â”ƒ
â”ƒ  â”‚    â†’ Calls pktgen_main_transmit() (registered via workq_add at startup)                              â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen_main_transmit() (pktgen.c:1339) - Determine next packet format                              â”‚   â”ƒ
â”ƒ  â”‚    Gets port-specific TX mempool: mp = l2p_get_tx_mp(pid) (pktgen.c:1348)                            â”‚   â”ƒ
â”ƒ  â”‚    â†’ Calls pktgen_send_pkts(pinfo, qid, mp)                                                           â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen_send_pkts() (pktgen.c:1307) - Get mbufs from mempool                                        â”‚   â”ƒ
â”ƒ  â”‚    API: rte_mempool_get_bulk(mp, (void **)pkts, txCnt) (pktgen.c:1323)                                â”‚   â”ƒ
â”ƒ  â”‚    â†’ Calls tx_send_packets()                                                                           â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  â€¢ tx_send_packets() (pktgen.c:463) - Core TX with retry logic                                        â”‚   â”ƒ
â”ƒ  â”‚    API: rte_eth_tx_burst(pid, qid, pkts, to_send) (pktgen.c:569)                                      â”‚   â”ƒ
â”ƒ  â”‚    - Retry loop handles partial sends (pktgen.c:567-572)                                              â”‚   â”ƒ
â”ƒ  â”‚      Partial send occurs when: elts[] full OR WQE Ring full (see N:M mapping in Key Takeaways)       â”‚   â”ƒ
â”ƒ  â”‚    - Tracks TX producer count & burst timing (AK)                                                      â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  [ALTERNATIVE FAST PATH - Zero-overhead mode]                                                          â”‚   â”ƒ
â”ƒ  â”‚  â€¢ fast_main_transmit() (pktgen.c:1360) - Optimized TX without special packets                        â”‚   â”ƒ
â”ƒ  â”‚    API: rte_mempool_get_bulk(mp, (void **)pkts, tx_burst) (pktgen.c:1367)                             â”‚   â”ƒ
â”ƒ  â”‚    API: rte_eth_tx_burst(pid, qid, pkts, send) (pktgen.c:1370)                                        â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  [RX PATH]                                                                                             â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen_main_receive() (pktgen.c:1391) - Main receive routine                                       â”‚   â”ƒ
â”ƒ  â”‚    Handles received packets and input processing                                                      â”‚   â”ƒ
â”ƒ  â”‚                                                                                                         â”‚   â”ƒ
â”ƒ  â”‚  [RATE LIMITING]                                                                                       â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Rate control logic - Control TX speed via TSC-based burst intervals                                â”‚   â”ƒ
â”ƒ  â”‚    Implemented in tx_send_packets()                                                                    â”‚   â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ                                                                                                           â”ƒ
â”ƒ  ğŸ’¾ Data Structures:                                                                                      â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ  â”‚  ğŸ“Š Temporary Working Buffers (NOT actual rings!)                                                 â”‚   â”ƒ
â”ƒ  â”‚                                                                                                    â”‚   â”ƒ
â”ƒ  â”‚  RX Working Buffer:                                                                               â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pinfo->rx_pkts[qid]  - Temporary array of mbuf pointers (pktgen-port-cfg.c:195)               â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Size: MAX_PKT_RX_BURST (typically 32)                                                          â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Purpose: Working space for rte_eth_rx_burst() to fill received packet pointers                â”‚   â”ƒ
â”ƒ  â”‚  â€¢ âš ï¸  NOT the actual RX ring! (actual ring created by rte_eth_rx_queue_setup in DPDK Core)      â”‚   â”ƒ
â”ƒ  â”‚                                                                                                    â”‚   â”ƒ
â”ƒ  â”‚  TX Working Buffer:                                                                               â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pinfo->tx_pkts[qid]  - Temporary array of mbuf pointers (pktgen-port-cfg.c:198)               â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Size: MAX_PKT_TX_BURST (typically 32)                                                          â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Populated by: rte_mempool_get_bulk() - gets mbuf pointers from DPDK Core mempool              â”‚   â”ƒ
â”ƒ  â”‚  â€¢ Consumed by: rte_eth_tx_burst() - passes pointers to DPDK Core for transmission               â”‚   â”ƒ
â”ƒ  â”‚  â€¢ âš ï¸  NOT the actual TX ring! (actual ring created by rte_eth_tx_queue_setup in DPDK Core)      â”‚   â”ƒ
â”ƒ  â”‚                                                                                                    â”‚   â”ƒ
â”ƒ  â”‚  âš™ï¸  Configuration (for DPDK Core layer rings, not application buffers):                          â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen.nb_rxd = DEFAULT_RX_DESC  (default 1024) - RX ring size in DPDK Core                   â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pktgen.nb_txd = DEFAULT_TX_DESC  (default 1024) - TX ring size in DPDK Core                   â”‚   â”ƒ
â”ƒ  â”‚    API: rte_eth_tx_queue_setup(pid, q, pktgen.nb_txd, ...) (pktgen-port-cfg.c:493)               â”‚   â”ƒ
â”ƒ  â”‚    API: rte_eth_rx_queue_setup(pid, q, pktgen.nb_rxd, ...) (pktgen-port-cfg.c:477)               â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pinfo->tx_burst - Number of packets per burst (typically 32) - working buffer size            â”‚   â”ƒ
â”ƒ  â”‚  â€¢ pinfo->rx_burst - Number of packets per RX burst (typically 32) - working buffer size         â”‚   â”ƒ
â”ƒ  â”‚                                                                                                    â”‚   â”ƒ
â”ƒ  â”‚  âœ… Key Insight: Application only holds pointers; actual memory allocated in DPDK Core            â”‚   â”ƒ
â”ƒ  â”‚     - Actual mbuf memory: allocated in DPDK Core Mempool                                          â”‚   â”ƒ
â”ƒ  â”‚     - Actual TX/RX rings: created in DPDK Core Ethdev/PMD layer                                   â”‚   â”ƒ
â”ƒ  â”‚     - Application: writes packet data via pointers; only holds temporary pointer arrays           â”‚   â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â”‚
                            â”‚ Application â†’ DPDK Core APIs:
                            â”‚ â€¢ rte_mempool_get_bulk() - Get mbuf pointers
                            â”‚ â€¢ rte_eth_tx_burst() / rx_burst() - Submit/receive packets
                            â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                                          DPDK CORE LIBRARIES LAYER (librte_* - Run-Time Environment)                                 â”ƒ
â”ƒ                                                                                                                                      â”ƒ
â”ƒ  ğŸ“ Code Location: dpdk/lib/                                                                                                        â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”ƒ
â”ƒ  â”‚  ğŸ“¦ MEMPOOL LIBRARY                  â”‚  ğŸ“‹ MBUF LIBRARY                         â”‚  ğŸ”Œ ETHDEV LIBRARY                          â”‚ â”ƒ
â”ƒ  â”‚  (Memory Pool Management)            â”‚  (Packet Buffer: Metadata + Data)        â”‚  (Ethernet Device Abstraction)              â”‚ â”ƒ
â”ƒ  â”‚  dpdk/lib/mempool/rte_mempool.h      â”‚  dpdk/lib/mbuf/rte_mbuf.h                â”‚  dpdk/lib/ethdev/rte_ethdev.h               â”‚ â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”ƒ
â”ƒ  â”‚  Concept:                            â”‚  Concept:                                â”‚  Concept:                                   â”‚ â”ƒ
â”ƒ  â”‚  Container                           â”‚  Object                                  â”‚  API Abstraction                            â”‚ â”ƒ
â”ƒ  â”‚  Stores objects efficiently          â”‚  Stored in mempool                       â”‚  Dispatches to PMD via function pointers    â”‚ â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”ƒ
â”ƒ  â”‚  Data Structure:                     â”‚  Data Structure:                         â”‚  Data Structure:                            â”‚ â”ƒ
â”ƒ  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ struct rte_mempool             â”‚  â”‚  â”‚ struct rte_mbuf                    â”‚  â”‚  â”‚ struct rte_eth_dev                    â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ (Name: TX-L1/P0/S0)            â”‚  â”‚  â”‚ (Size: 2,176 bytes each)           â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ created by lcore1, but shared  â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚ Metadata Only (No packet storage!):   â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”“  â”‚  â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“   â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”ƒ mbuf â”ƒ mbuf â”ƒ mbuf â”ƒ mbuf â”ƒ  â”‚  â”‚  â”‚ â”ƒ Metadata Section:            â”ƒ   â”‚  â”‚  â”‚ â€¢ tx_pkt_burst  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”ƒ  0   â”ƒ  1   â”ƒ  2   â”ƒ  3   â”ƒ  â”‚  â”‚  â”‚ â”ƒ â€¢ packet_length              â”ƒ   â”‚  â”‚  â”‚   mlx5_tx_burst()                     â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”—â”â”â”â”â”â”â”»â”â”â”â”â”â”â”»â”â”â”â”â”â”â”»â”â”â”â”â”â”â”›  â”‚  â”‚  â”‚ â”ƒ â€¢ data_offset                â”ƒ   â”‚  â”‚  â”‚   (function pointer)                  â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”“  â”‚  â”‚  â”‚ â”ƒ â€¢ reference_count (refcnt)   â”ƒ   â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”ƒ mbuf â”ƒ mbuf â”ƒ  ...  â”ƒ mbufâ”ƒ  â”‚  â”‚  â”‚ â”ƒ â€¢ offload_flags              â”ƒ   â”‚  â”‚  â”‚ â€¢ rx_pkt_burst  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”ƒ  4   â”ƒ  5   â”ƒ       â”ƒ  N  â”ƒ  â”‚  â”‚  â”‚ â”ƒ â€¢ mbuf_mempool pointer       â”ƒ   â”‚  â”‚  â”‚   mlx5_rx_burst()                     â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”—â”â”â”â”â”â”â”»â”â”â”â”â”â”â”»â”â”â”â”â”â”â”»â”â”â”â”â”â”â”›  â”‚  â”‚  â”‚ â”ƒ                              â”ƒ   â”‚  â”‚  â”‚   (function pointer)                  â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â”ƒ Packet Data Section:         â”ƒ   â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ Total: 32,768 mbufs            â”‚  â”‚  â”‚ â”ƒ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”ƒ   â”‚  â”‚  â”‚ â€¢ Port configuration:                 â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â”ƒ â”‚ Ethernet | IP | TCP | ...â”‚ â”ƒ   â”‚  â”‚  â”‚   - Link speed, duplex                â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   get_bulk()                   â”‚  â”‚  â”‚ â”ƒ â”‚ (Actual network packet)  â”‚ â”ƒ   â”‚  â”‚  â”‚   - MTU size                          â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   Called by Application        â”‚  â”‚  â”‚ â”ƒ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”ƒ   â”‚  â”‚  â”‚   - Offload capabilities              â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â”ƒ (up to 2,048 bytes)          â”ƒ   â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   put_bulk()                   â”‚  â”‚  â”‚ â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›   â”‚  â”‚  â”‚ â€¢ Queue metadata:                     â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   Called by PMD (on completion)â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚   - Number of RX queues               â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ ğŸ“¦ Detailed Memory Layout:         â”‚  â”‚  â”‚   - Number of TX queues               â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ ğŸ”’ Per-core Cache (Lock-free): â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚   - Queue descriptors                 â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”‚ Core 0 Cache               â”‚ â”‚  â”‚  â”‚ â”‚ struct rte_mbuf (128B)         â”‚ â”‚  â”‚  â”‚ ğŸ“Š Stats Collection:                  â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”‚ [mbuf][mbuf]...[mbuf]      â”‚ â”‚  â”‚  â”‚ â”‚ â€¢ buf_addr â†’ data start        â”‚ â”‚  â”‚  â”‚ â€¢ rte_eth_stats_get()                 â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚ â”‚ â€¢ buf_len = 2048               â”‚ â”‚  â”‚  â”‚   - ipackets, opackets                â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚ â”‚ â€¢ data_len = actual pkt size   â”‚ â”‚  â”‚  â”‚   - ibytes, obytes                    â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”‚ Core 1 Cache               â”‚ â”‚  â”‚  â”‚ â”‚ â€¢ pkt_len = total length       â”‚ â”‚  â”‚  â”‚   - ierrors, oerrors                  â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â”‚ [mbuf][mbuf]...[mbuf]      â”‚ â”‚  â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚  â”‚   - rx_nombuf (out of mbufs!)         â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚ â”‚ Headroom (128B)                â”‚ â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚  â”‚ ğŸ›ï¸ Device Control:                    â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ Performance:                   â”‚  â”‚  â”‚ â”‚         â†‘ buf_addr points here â”‚ â”‚  â”‚  â”‚ â€¢ rte_eth_dev_start()                 â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ get_bulk from cache: ~10 ns  â”‚  â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚  â”‚ â€¢ rte_eth_dev_stop()                  â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ Cache miss â†’ main pool:      â”‚  â”‚  â”‚ â”‚ Packet Data (0-2048B)          â”‚ â”‚  â”‚  â”‚ â€¢ rte_eth_promiscuous_enable()        â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   ~100 ns (lock contention)    â”‚  â”‚  â”‚ â”‚ â€¢ NIC DMA reads from here      â”‚ â”‚  â”‚  â”‚ â€¢ rte_eth_link_get_nowait()           â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ Cache Size Tuning:             â”‚  â”‚  â”‚ â”‚ Tailroom                       â”‚ â”‚  â”‚  â”‚ âš¡ Offload Capabilities:              â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ Default: 256 mbufs/core      â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚ â€¢ TX: Checksums, TSO, VLAN            â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ Larger cache â†’ less lock     â”‚  â”‚  â”‚ Total: ~2304 bytes per mbuf       â”‚  â”‚  â”‚ â€¢ RX: Checksum validation             â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   contention, more memory      â”‚  â”‚  â”‚ (single contiguous allocation)    â”‚  â”‚  â”‚ â€¢ Multi-segment packets               â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ Smaller cache â†’ memory       â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚ â€¢ RSS (Receive Side Scaling)          â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   efficient, more contention   â”‚  â”‚  â”‚ Key Points:                        â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â€¢ Metadata + data in ONE block     â”‚  â”‚  â”‚ ğŸ”„ Burst Operations:                  â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ âš ï¸ Out of mbufs?                â”‚  â”‚  â”‚ â€¢ buf_addr = mbuf_addr +           â”‚  â”‚  â”‚ â€¢ Batch size: 32-64 typical           â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â†’ Check rx_nombuf stat!        â”‚  â”‚  â”‚             sizeof(mbuf) + 128     â”‚  â”‚  â”‚ â€¢ Amortizes function call cost        â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â€¢ On free: only metadata touched   â”‚  â”‚  â”‚ â€¢ Improves cache locality             â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚   (~128B/mbuf), pkt data NOT       â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚ â€¢ Full 2KB cached during TX only,  â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚   not during free                  â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ Features:                      â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ Per-port shared mempool      â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   (all lcores using that port) â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ â€¢ Per-core cache for           â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚ â„¹ï¸  Pure metadata - no actual         â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   performance optimization     â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚    packet data stored here!           â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚   (MEMPOOL_CACHE_SIZE)         â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚                                â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â”‚ âš ï¸ Potential contention point! â”‚  â”‚  â”‚                                    â”‚  â”‚  â”‚                                       â”‚  â”‚ â”ƒ
â”ƒ  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”ƒ
â”ƒ  â”‚  ğŸ”§ API Usage Flow:                  â”‚  ğŸ”§ API Usage:                           â”‚  ğŸ”§ API Usage Flow:                         â”‚ â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”ƒ
â”ƒ  â”‚  [INITIALIZATION - Once at startup]  â”‚  [SLOW PATH - NOT Pktgen fast path!]     â”‚  [FAST PATH TX - Every burst]               â”‚ â”ƒ
â”ƒ  â”‚  Application calls:                  â”‚                                          â”‚  Application calls:                         â”‚ â”ƒ
â”ƒ  â”‚  rte_mempool_create(                 â”‚  rte_pktmbuf_alloc(mempool)              â”‚  rte_eth_tx_burst(                          â”‚ â”ƒ
â”ƒ  â”‚    "TX-L1/P0/S0",                    â”‚  â€¢ Allocate single mbuf                  â”‚    port_id, queue_id,                       â”‚ â”ƒ
â”ƒ  â”‚    32768,                            â”‚  â€¢ Slower than bulk operations           â”‚    mbufs[], 32)                             â”‚ â”ƒ
â”ƒ  â”‚    sizeof(struct rte_mbuf),          â”‚                                          â”‚  (pktgen.c:569)                             â”‚ â”ƒ
â”ƒ  â”‚    ...)                              â”‚  rte_pktmbuf_free(mbuf)                  â”‚  â†’ Generic API (works for any NIC driver)   â”‚ â”ƒ
â”ƒ  â”‚  â†’ Creates pool with 32,768 mbufs    â”‚  â€¢ Free single mbuf                      â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚                                      â”‚  â€¢ Slower than bulk operations           â”‚  â†’ Internally dispatches to:                â”‚ â”ƒ
â”ƒ  â”‚  [FAST PATH TX - Every burst]        â”‚                                          â”‚    dev->tx_pkt_burst(...)                   â”‚ â”ƒ
â”ƒ  â”‚  Application (Pktgen) calls:         â”‚  âš ï¸  Pktgen does NOT use these APIs!     â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  rte_mempool_get_bulk(               â”‚     Uses mempool bulk APIs instead       â”‚  â†’ Calls PMD-specific implementation:       â”‚ â”ƒ
â”ƒ  â”‚    mempool, mbufs[], 32)             â”‚     for better performance!              â”‚    mlx5_tx_burst()                          â”‚ â”ƒ
â”ƒ  â”‚  (pktgen.c:1323)                     â”‚                                          â”‚    via function pointer                     â”‚ â”ƒ
â”ƒ  â”‚  â†’ Gets 32 pre-filled mbufs          â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  â†’ Zero-copy reuse pattern!          â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚                                      â”‚                                          â”‚  â†’ PMD writes to hardware TX queue          â”‚ â”ƒ
â”ƒ  â”‚  [COMPLETION PATH - After TX]        â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  PMD (MLX5 driver) calls:            â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  rte_mempool_put_bulk(               â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚    mempool, mbufs[], 32)             â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  (mlx5_tx.h:566 or :621)             â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  â†’ Returns 32 completed mbufs        â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â”‚  â†’ Back to pool for reuse            â”‚                                          â”‚                                             â”‚ â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”ƒ
â”ƒ                                                                                        â”ƒ
â”ƒ  âœ¨ Key Insight: Relationship between Mempool and mbuf                                 â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”ƒ
â”ƒ  â”‚  Mempool = Parking Lot (Container)     |  mbuf = Car (Object)                   â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Stores objects efficiently          |  â€¢ Packet data + metadata              â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Allocation/deallocation management  |  â€¢ Stored in Mempool                   â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Performance optimization via        |  â€¢ 2,176 bytes per mbuf                â”‚  â”ƒ
â”ƒ  â”‚    per-core cache                      |                                        â”‚  â”ƒ
â”ƒ  â”‚                                                                                  â”‚  â”ƒ
â”ƒ  â”‚  "mbuf Mempool" = Mempool that stores mbuf-type objects                         â”‚  â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â”‚
                            â”‚ DPDK Core â‡„ PMD APIs:
                            â”‚ â€¢ DPDK Core â†’ PMD: rte_eth_tx_burst() dispatches to mlx5_tx_burst()
                            â”‚ â€¢ PMD â†’ DPDK Core: rte_mempool_put_bulk() returns completed mbufs
                            â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                    PMD LAYER (MLX5 Driver)                          â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  ğŸ“ Code Location: dpdk/drivers/net/mlx5/mlx5_tx.h                 â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”ƒ
â”ƒ  â”‚  ğŸ”§ TX Functions:                                            â”‚ â”ƒ
â”ƒ  â”‚  â€¢ mlx5_tx_burst()          - Main TX entry point           â”‚ â”ƒ
â”ƒ  â”‚  â€¢ mlx5_tx_handle_          - Process completions           â”‚ â”ƒ
â”ƒ  â”‚    completion()                                              â”‚ â”ƒ
â”ƒ  â”‚  â€¢ mlx5_tx_free_elts()      - Free completed mbufs          â”‚ â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”ƒ
â”ƒ  â”‚  ğŸ›ï¸  struct mlx5_txq_data (mlx5_tx.h:123-186)                  â”‚â”ƒ
â”ƒ  â”‚  Software TX queue control structure - manages both arrays     â”‚â”ƒ
â”ƒ  â”‚                                                                 â”‚â”ƒ
â”ƒ  â”‚  Management Fields (indices for tracking):                     â”‚â”ƒ
â”ƒ  â”‚    â€¢ wqe_ci, wqe_pi  - WQE Ring consumer/producer indices      â”‚â”ƒ
â”ƒ  â”‚    â€¢ elts_head, elts_tail - elts[] head/tail indices           â”‚â”ƒ
â”ƒ  â”‚    â€¢ wqe_s, elts_s   - Ring/array sizes                        â”‚â”ƒ
â”ƒ  â”‚                                                                 â”‚â”ƒ
â”ƒ  â”‚  Pointers to Separate Arrays:                                  â”‚â”ƒ
â”ƒ  â”‚    â€¢ struct mlx5_wqe *wqes;   â†’ WQE Ring (SQ, for hardware)    â”‚â”ƒ
â”ƒ  â”‚    â€¢ struct rte_mbuf *elts[]; â†’ mbuf tracking (for software)   â”‚â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”ƒ
â”ƒ  â”‚  âš™ï¸  WQE Ring (Send Queue)           â”‚  ğŸ“Œ elts[] Array                    â”‚â”ƒ
â”ƒ  â”‚  Hardware Work Descriptors Queue     â”‚     Software mbuf Tracking          â”‚â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”ƒ
â”ƒ  â”‚  Pointed to by: txq->wqes            â”‚  Pointed to by: txq->elts           â”‚â”ƒ
â”ƒ  â”‚  (mlx5_tx.h:163)                     â”‚  (mlx5_tx.h:184)                    â”‚â”ƒ
â”ƒ  â”‚                                      â”‚                                     â”‚â”ƒ
â”ƒ  â”‚  Purpose:                            â”‚  Purpose:                           â”‚â”ƒ
â”ƒ  â”‚  NIC reads these descs via DMA to    â”‚  Driver tracks mbuf pointers        â”‚â”ƒ
â”ƒ  â”‚  execute TX operations               â”‚  to free them on TX completion      â”‚â”ƒ
â”ƒ  â”‚                                      â”‚                                     â”‚â”ƒ
â”ƒ  â”‚  â”â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”“          â”‚  â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”“      â”‚â”ƒ
â”ƒ  â”‚  â”ƒWQE0 â”ƒWQE1 â”ƒWQE2 â”ƒWQE3 â”ƒ ...      â”‚  â”ƒmbuf0*â”ƒmbuf1*â”ƒmbuf2*â”ƒmbuf3*â”ƒ ... â”‚â”ƒ
â”ƒ  â”‚  â”—â”â”â”â”â”â”»â”â”â”â”â”â”»â”â”â”â”â”â”»â”â”â”â”â”â”›          â”‚  â”—â”â”â”â”â”â”â”»â”â”â”â”â”â”â”»â”â”â”â”â”â”â”»â”â”â”â”â”â”â”›      â”‚â”ƒ
â”ƒ  â”‚  (size: DEFAULT_TX_DESC)             â”‚  (size: DEFAULT_TX_DESC)            â”‚â”ƒ
â”ƒ  â”‚                                      â”‚                                     â”‚â”ƒ
â”ƒ  â”‚  Each WQE contains HW desc:          â”‚  Each element stores:               â”‚â”ƒ
â”ƒ  â”‚  â€¢ pbuf (DMA address)                â”‚  â€¢ struct rte_mbuf* pointer         â”‚â”ƒ
â”ƒ  â”‚    - where NIC reads packet data    â”‚  â€¢ Used to return mbuf to mempool   â”‚â”ƒ
â”ƒ  â”‚  â€¢ bcount - bytes to transmit        â”‚    on completion                    â”‚â”ƒ
â”ƒ  â”‚  â€¢ lkey - memory region key          â”‚                                     â”‚â”ƒ
â”ƒ  â”‚    (for NIC DMA access permission)   â”‚                                     â”‚â”ƒ
â”ƒ  â”‚                                      â”‚                                     â”‚â”ƒ
â”ƒ  â”‚  âš ï¸  WQE = work instruction          â”‚  Why needed:                        â”‚â”ƒ
â”ƒ  â”‚  for NIC, NOT packet data itself!    â”‚  â€¢ NIC only knows DMA addresses     â”‚â”ƒ
â”ƒ  â”‚  For hardware (NIC DMA reads).       â”‚  â€¢ SW (PMD) needs virtual pointers  â”‚â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  ğŸ’¾ Data Structures:                                               â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  ğŸ“ Size Determination (mlx5_txq.c:1135, 1168, 708):               â”ƒ
â”ƒ  â€¢ By DEFAULT_TX_DESC (Default: 1024)                              â”ƒ
â”ƒ  â€¢ Both arrays sized from same desc parameter:                     â”ƒ
â”ƒ    - elts[] size = desc (mlx5_txq.c:1168)                          â”ƒ
â”ƒ    - WQE Ring size = 1 << log2above(desc) (mlx5_txq.c:708)         â”ƒ
â”ƒ  â€¢ âš ï¸  desc is rounded up to nearest power of 2!                   â”ƒ
â”ƒ    (e.g., desc=1000 â†’ WQE Ring=1024, elts[]=1024)                  â”ƒ
â”ƒ  â€¢ âš ï¸  SAME SIZE, BUT N:M ELEMENT MAPPING!                         â”ƒ
â”ƒ    â€¢ NOT 1:1 element-wise correspondence!                          â”ƒ
â”ƒ      â€¢ No inline mode (eMPW - Enhanced Multi-Packet Write):        â”ƒ
â”ƒ        - 1 eMPW session uses multiple WQEBBs (typically 2)         â”ƒ
â”ƒ        - 1st WQEBB: Control(16B) + Ethernet(16B) + 2 dsegs(32B)   â”ƒ
â”ƒ        - 2nd WQEBB: 4 dsegs(64B) - pure data segments              â”ƒ
â”ƒ        - Typical: 2+2 = 4 packets (optimal for latency)            â”ƒ
â”ƒ        - Can use more WQEBBs â†’ up to 32 pkts/session (soft limit)  â”ƒ
â”ƒ        - With 2 WQEBBs: Max 2+4 = 6 packets (but causes spikes)    â”ƒ
â”ƒ      â€¢ Inline mode (MPW_INLINE): 1 elts[] can span multiple WQEs   â”ƒ
â”ƒ        - Max 6 packets per session (hard limit, data size bound)   â”ƒ
â”ƒ        - Large packets need multiple WQEBBs for inlined data       â”ƒ
â”ƒ    â€¢ Independent consumption rates (see detailed section below)    â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”ƒ
â”ƒ  ğŸ§± WQEBB (Work Queue Element Building Block)                      â”ƒ
â”ƒ  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  WQEBB = Basic unit of WQE Ring allocation (fixed 64 bytes)        â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Key Concepts:                                                      â”ƒ
â”ƒ    â€¢ WQEBB (Building Block):  64 bytes (4 segments Ã— 16B)          â”ƒ
â”ƒ    â€¢ WQE (Work Element):      1 or more WQEBBs                     â”ƒ
â”ƒ  WQE Size Examples:                                                 â”ƒ
â”ƒ    â€¢ Small WQE:   1 WQEBB  (64B)  - single small packet           â”ƒ
â”ƒ    â€¢ Medium WQE:  2 WQEBBs (128B) - typical eMPW session          â”ƒ
â”ƒ    â€¢ Large WQE:   3+ WQEBBs (192B+) - inline mode or many packets â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Why WQEBB?                                                         â”ƒ
â”ƒ    â€¢ Fixed-size blocks â†’ easier ring buffer management             â”ƒ
â”ƒ    â€¢ Variable-size WQEs built from fixed blocks                    â”ƒ
â”ƒ    â€¢ Simplifies wraparound calculation at ring boundary            â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  WQE Ring = Array of WQEBBs:                                        â”ƒ
â”ƒ    [WQEBB 0][WQEBB 1][WQEBB 2][WQEBB 3]...[WQEBB 1023]            â”ƒ
â”ƒ    â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”ƒ
â”ƒ     WQE #1   WQE #2 (2 WQEBBs)    ...                              â”ƒ
â”ƒ    (1 WQEBB)                                                       â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  âš ï¸  WQE Ring Size vs Actual Session Count:                        â”ƒ
â”ƒ    WQE Ring size = 1024 means:                                     â”ƒ
â”ƒ      â†’ 1024 WQEBBs (64B Ã— 1024 = 64KB total space)                â”ƒ
â”ƒ      â†’ NOT 1024 WQE sessions!                                      â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Actual WQE session count = VARIABLE (depends on actual WQE sizes): â”ƒ
â”ƒ      Case 1: Each WQE = 1 WQEBB (single small packet)              â”ƒ
â”ƒ        â†’ Max 1024 sessions                                         â”ƒ
â”ƒ      Case 2: Each WQE = 2 WQEBBs (typical eMPW)                    â”ƒ
â”ƒ        â†’ Max 512 sessions                                          â”ƒ
â”ƒ      Case 3: Each WQE = 4 WQEBBs (large inline)                    â”ƒ
â”ƒ        â†’ Max 256 sessions                                          â”ƒ
â”ƒ      Case 4: Mixed sizes (runtime determined)                      â”ƒ
â”ƒ        â†’ Example: [1 WQEBB][2 WQEBBs][2 WQEBBs][3 WQEBBs]...       â”ƒ
â”ƒ        â†’ 6 WQEBBs used â†’ 4 sessions created                        â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ    Key Insight - N:M Mapping:                                      â”ƒ
â”ƒ      â€¢ WQE Ring: 1024 WQEBBs (fixed allocation units)              â”ƒ
â”ƒ      â€¢ elts[] array: 1024 entries (fixed packet slots)             â”ƒ
â”ƒ      â€¢ Actual WQE sessions: Variable (< 1024)                      â”ƒ
â”ƒ      â€¢ Each WQE can reference multiple elts[] entries              â”ƒ
â”ƒ      â€¢ Example: 512 WQE sessions Ã— 2 pkts/session = 1024 packets  â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”ƒ
â”ƒ  ğŸ“Š eMPW Session Structure Details (Enhanced Multi-Packet Write)   â”ƒ
â”ƒ  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”ƒ
â”ƒ  eMPW enables batching multiple packets in a single WQE session    â”ƒ
â”ƒ  by using multiple WQEBBs. This reduces doorbell writes overhead   â”ƒ
â”ƒ  and improves efficiency for small packets.                        â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ  â”‚ 1st WQEBB (64 bytes)                                        â”‚   â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”ƒ
â”ƒ  â”‚ Control Segment     (16 bytes) â† WQE header                â”‚   â”ƒ
â”ƒ  â”‚ Ethernet Segment    (16 bytes) â† Ethernet header           â”‚   â”ƒ
â”ƒ  â”‚ Data Segment 0      (16 bytes) â† Packet 1 pointer          â”‚   â”ƒ
â”ƒ  â”‚   â€¢ bcount(4B) + lkey(4B) + pbuf(8B)                       â”‚   â”ƒ
â”ƒ  â”‚ Data Segment 1      (16 bytes) â† Packet 2 pointer          â”‚   â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”ƒ
â”ƒ  â”‚ 2nd+ WQEBB (64 bytes) - "one WQEBB may contains up to      â”‚   â”ƒ
â”ƒ  â”‚                          four packets" (mlx5_tx.h:3721)    â”‚   â”ƒ
â”ƒ  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”ƒ
â”ƒ  â”‚ Data Segment 0      (16 bytes) â† Packet 3 pointer          â”‚   â”ƒ
â”ƒ  â”‚ Data Segment 1      (16 bytes) â† Packet 4 pointer          â”‚   â”ƒ
â”ƒ  â”‚ Data Segment 2      (16 bytes) â† Packet 5 pointer          â”‚   â”ƒ
â”ƒ  â”‚ Data Segment 3      (16 bytes) â† Packet 6 pointer          â”‚   â”ƒ
â”ƒ  â”‚ (Pure data segments - no headers! Up to 4 pkts/WQEBB)      â”‚   â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”ƒ
â”ƒ  Packet Capacity Calculation:                                      â”ƒ
â”ƒ    â€¢ 1st WQEBB: 2 packets (limited by Control+Ethernet headers)   â”ƒ
â”ƒ    â€¢ 2nd+ WQEBB: Up to 4 packets each (pure data segments)        â”ƒ
â”ƒ    â€¢ 2 WQEBBs: 2 + 4 = 6 packets max                              â”ƒ
â”ƒ    â€¢ Can use more WQEBBs up to MLX5_EMPW_MAX_PACKETS = 32         â”ƒ
â”ƒ  Code Evidence:                                                   â”ƒ
â”ƒ    â€¢ mlx5_tx.h:3017-3020 - Calculates available room across       â”ƒ
â”ƒ      multiple WQEBBs minus Control & Ethernet segments            â”ƒ
â”ƒ    â€¢ mlx5_tx.h:3177,3181,3183 - Iterates through data segments:   â”ƒ
â”ƒ        mlx5_tx_dseg_ptr() â†’ room -= MLX5_WQE_DSEG_SIZE â†’ ++dseg   â”ƒ
â”ƒ    â€¢ mlx5_prm.h:107 - MLX5_MPW_INLINE_MAX_PACKETS = 6             â”ƒ
â”ƒQ: Why "up to 4 packets" instead of max 6?                          â”ƒ
â”ƒA: Sweet spot to prevent latency spikes during completion processingâ”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Key Constants (mlx5_defs.h:23, mlx5_prm.h:105-107):               â”ƒ
â”ƒ    â€¢ MLX5_TX_COMP_THRESH = 32                                      â”ƒ
â”ƒ      â†’ PMD sets completion flag in WQE Control Segment every       â”ƒ
â”ƒ        32 descriptors (mlx5_tx.h:771-782)                          â”ƒ
â”ƒ      â†’ Tells NIC: "generate CQE when this WQE completes"           â”ƒ
â”ƒ      â†’ Reduces CQ overhead (not every WQE needs CQE)                â”ƒ
â”ƒ    â€¢ MLX5_TX_COMP_MAX_CQE = 2                                      â”ƒ
â”ƒ      â†’ Max completion entries processed per tx_burst() call        â”ƒ
â”ƒ      â†’ Limits burst size when freeing mbufs                        â”ƒ
â”ƒ    â€¢ MLX5_EMPW_MAX_PACKETS = 32 (mlx5_prm.h:105)                   â”ƒ
â”ƒ      â†’ Max packets in each eMPW session (pointer mode, no inline)  â”ƒ
â”ƒ      â†’ Soft limit = MLX5_TX_COMP_THRESH (completion threshold)     â”ƒ
â”ƒ      â†’ Can use many WQEBBs (each packet = 16B data segment)        â”ƒ
â”ƒ      â†’ Prevents unbounded mbuf accumulation before completion      â”ƒ
â”ƒ    â€¢ MLX5_MPW_INLINE_MAX_PACKETS = 6 (mlx5_prm.h:107)              â”ƒ
â”ƒ      â†’ Max packets PER MPW SESSION (mlx5_tx.h:2976-2977)          â”ƒ
â”ƒ      â†’ "one WQE" = one session with Control+Ethernet headers      â”ƒ
â”ƒ      â†’ Inline mode: actual packet data copied to WQE              â”ƒ
â”ƒ      â†’ Hard limit to improve CQE latency generation               â”ƒ
â”ƒ      â†’ Example: 6 Ã— 64B packets â†’ ~384B data â†’ 6+ WQEBBs/session â”ƒ
â”ƒ      â†’ For more packets: create multiple sessions                 â”ƒ
â”ƒ      â†’ Data segment union [16B]:                                        â”ƒ
â”ƒ       bcount[4B] + (inline_data[12B] OR lkey+pbuf[12B]             â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Completion Burst Analysis (mlx5_prm.h:98-105):                    â”ƒ
â”ƒ    Scenario        â”‚ CQEs â”‚ Packets/CQE â”‚ mbufs freed â”‚ Result     â”ƒ
â”ƒ    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”ƒ
â”ƒ    6 pkts/session  â”‚  2   â”‚      6      â”‚     12      â”‚ âŒ Spike!  â”ƒ
â”ƒ    4 pkts/session  â”‚  2   â”‚      4      â”‚      8      â”‚ âœ… Good    â”ƒ
â”ƒ    2 pkts/session  â”‚  2   â”‚      2      â”‚      4      â”‚ âš ï¸  Waste  â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Why 4 = Sweet Spot:                                               â”ƒ
â”ƒ    âœ“ Predictable latency: 8 mbufs freed per burst (manageable)    â”ƒ
â”ƒ    âœ“ WQE efficiency: Uses 2 WQEBBs (reasonable overhead)           â”ƒ
â”ƒ    âœ“ Avoids completion burst: Code comment warns about             â”ƒ
â”ƒ      "significant latency" if too many mbufs freed at once         â”ƒ
â”ƒ    âœ“ Cache working set pressure management:                        â”ƒ
â”ƒ      â€¢ TX path hot data: descriptors, CQ, mempool, next packets    â”ƒ
â”ƒ      â€¢ Larger burst â†’ more cache pressure â†’ evict other hot data   â”ƒ
â”ƒ      â€¢ Next packet processing may see cache misses                 â”ƒ
â”ƒ      â€¢ 8 mbufs manageable, 12 mbufs risks evicting critical data   â”ƒ
â”ƒ    âœ— Wastes 32B: 2nd WQEBB half unused, but stability > space     â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Trade-off Decision:                                               â”ƒ
â”ƒ    Sacrifice 32 bytes per session â†’ Gain consistent performance    â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  Key Insight: This explains N:M mapping!                           â”ƒ
â”ƒ    â€¢ 1-2 WQEBBs consumed from WQE Ring                             â”ƒ
â”ƒ    â€¢ 4-6 mbuf pointers stored in elts[] array                      â”ƒ
â”ƒ    â€¢ Different consumption rates between WQE ring and elts array   â”ƒ
â”ƒ     â†’ must check availability of both resources                     â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”ƒ
â”ƒ  â”‚  âœ… Completion Queue (CQ)                                      â”‚â”ƒ
â”ƒ  â”‚  â€¢ Hardware writes completion status here                     â”‚â”ƒ
â”ƒ  â”‚  â€¢ Driver polls CQ to free mbufs (no interrupts)              â”‚â”ƒ
â”ƒ  â”‚  â€¢ Updates wqe_ci when packets complete                       â”‚â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  âš ï¸  IMPORTANT: Physical Memory Location                           â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”ƒ
â”ƒ  â”‚  WQE Ring & CQ Location: HOST MEMORY (NOT on NIC chip!)       â”‚â”ƒ
â”ƒ  â”‚                                                                 â”‚â”ƒ
â”ƒ  â”‚  Evidence: drivers/common/mlx5/mlx5_common_devx.c:118-127      â”‚â”ƒ
â”ƒ  â”‚  ```                                                            â”‚â”ƒ
â”ƒ  â”‚  /* Allocate memory buffer for CQEs and doorbell record. */    â”‚â”ƒ
â”ƒ  â”‚  umem_size = sizeof(struct mlx5_cqe) * num_of_cqes;            â”‚â”ƒ
â”ƒ  â”‚  umem_buf = mlx5_malloc_numa_tolerant(..., socket);            â”‚â”ƒ
â”ƒ  â”‚                    â†‘                                            â”‚â”ƒ
â”ƒ  â”‚            Host memory allocation!                              â”‚â”ƒ
â”ƒ  â”‚                                                                 â”‚â”ƒ
â”ƒ  â”‚  /* Register allocated buffer with DevX for DMA access */      â”‚â”ƒ
â”ƒ  â”‚  umem_obj = mlx5_os_umem_reg(ctx, umem_buf, umem_size,         â”‚â”ƒ
â”ƒ  â”‚                               IBV_ACCESS_LOCAL_WRITE);          â”‚â”ƒ
â”ƒ  â”‚                    â†‘                                            â”‚â”ƒ
â”ƒ  â”‚            DMA-capable memory registration                      â”‚â”ƒ
â”ƒ  â”‚  ```                                                            â”‚â”ƒ
â”ƒ  â”‚                                                                 â”‚â”ƒ
â”ƒ  â”‚  Memory Architecture:                                           â”‚â”ƒ
â”ƒ  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚â”ƒ
â”ƒ  â”‚  â”‚ HOST MEMORY (RAM)                                    â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ WQE Ring (SQ)      â”‚ â† PMD allocates              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ - DMA-mapped       â”‚   (Host RAM)                 â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ - NIC reads (DMA)  â”‚                              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚          â†‘ DMA Read                                  â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ Completion Queue   â”‚ â† PMD allocates              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ - DMA-mapped       â”‚   (Host RAM)                 â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ - NIC writes (DMA) â”‚                              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚      â”‚â”ƒ
â”ƒ  â”‚  â”‚          â†‘ DMA Write                                 â”‚      â”‚â”ƒ
â”ƒ  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚â”ƒ
â”ƒ  â”‚             â”‚                                                   â”‚â”ƒ
â”ƒ  â”‚             â”‚ PCIe Bus (DMA transfers)                          â”‚â”ƒ
â”ƒ  â”‚             â†“                                                   â”‚â”ƒ
â”ƒ  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚â”ƒ
â”ƒ  â”‚  â”‚ NIC HARDWARE (On-chip)                              â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ Internal FIFO    â”‚ â† Software cannot access      â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  â”‚ (HW-only buffer) â”‚   (NIC internal only)         â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  TX Engine (DMA controller)                         â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  - Reads WQEs from host memory via DMA              â”‚       â”‚â”ƒ
â”ƒ  â”‚  â”‚  - Writes CQEs to host memory via DMA               â”‚       â”‚â”ƒ
â”ƒ  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚â”ƒ
â”ƒ  â”‚                                                                 â”‚â”ƒ
â”ƒ  â”‚  Key Distinction:                                               â”‚â”ƒ
â”ƒ  â”‚  â€¢ WQE Ring/CQ: Host memory (DMA-mapped, PMD manages)          â”‚â”ƒ
â”ƒ  â”‚  â€¢ On-chip FIFO: NIC chip (HW-only, not accessible by SW)      â”‚â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            â”‚
                            â”‚ ğŸš€ DMA Transfer & Doorbell Ring
                            â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                     HARDWARE LAYER (NIC)                            â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  ğŸ”Œ Device: Mellanox ConnectX-5 (MLX5)                            â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”ƒ
â”ƒ  â”‚  ğŸš€ TX Engine:                                               â”‚ â”ƒ
â”ƒ  â”‚  â€¢ Reads WQEs via DMA      - Fetch descriptors               â”‚ â”ƒ
â”ƒ  â”‚  â€¢ Fetches packet data     - DMA from mbuf buffers           â”‚ â”ƒ
â”ƒ  â”‚  â€¢ Transmits to wire       - Physical layer transmission     â”‚ â”ƒ
â”ƒ  â”‚  â€¢ Writes completions      - Update completion queue         â”‚ â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”ƒ
â”ƒ                                                                     â”ƒ
â”ƒ  ğŸ’¾ Hardware Queues:                                               â”ƒ
â”ƒ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”ƒ
â”ƒ  â”‚  âš¡ TX Queue (on NIC)                                      â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Hardware FIFO                                           â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Processes WQEs in order                                 â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Multiple queues supported (multi-queue/RSS)             â”‚  â”ƒ
â”ƒ  â”‚  â€¢ Size: Hardware-dependent                                â”‚  â”ƒ
â”ƒ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”ƒ
â”ƒ                                 â†“                                  â”ƒ
â”ƒ                        ğŸŒ [ Wire / Network ]                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
```

---

## 3. Data Structure Ownership & Lifecycle

### Overview Table

| Layer | Data Structures | Purpose | Typical Size |
|-------|-----------------|---------|--------------|
| **Application<br>(Pktgen)** | **Application RX/TX Rings**<br>(optional) | Software rings storing mbuf ptrs<br>Buffer packets before TX/after RX | Configurable<br>(nb_rxd/nb_txd) |
| **DPDK Core<br>Libraries** | **mbuf Mempool**<br>(TX-L1/P0/S0) | Packet buffer allocation/free<br>Pre-allocated mbufs<br>Shared across cores<br>Per-core cache | 32,768 mbufs<br>(configurable) |
| | **struct rte_mbuf** | Packet metadata & data buffer<br>Reference counting, offloads | 2176 bytes/mbuf |
| | **struct rte_eth_dev** | Function pointers to PMD<br>Port/Queue configuration<br>**No packet storage!** | Metadata only |
| **PMD<br>(MLX5)** | **WQE Ring Buffer** | Hardware descriptors<br>DMA-mapped memory<br>NIC reads directly | wqe_s<br>(e.g., 1024) |
| | **SW TX Queue**<br>(mlx5_txq_data) | Tracks mbuf pointers (elts[])<br>Queue indices (wqe_ci, wqe_pi)<br>Queue depth calculation | Same as WQE ring |
| | **Completion Queue** | Hardware completion notifications<br>Polled by driver | Configurable |
| **Hardware<br>(NIC)** | **On-chip TX FIFO** | Internal buffering<br>Not accessible by software | HW-dependent |

### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Structure Lifecycle                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  [1] ALLOCATION (Initialization Phase)
      Application calls rte_mempool_create()
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ DPDK Core Libraries (Mempool)          â”‚
      â”‚  mbuf Mempool created (TX-L1/P0/S0)    â”‚
      â”‚  â€¢ Pre-allocates 32,768 mbufs          â”‚
      â”‚  â€¢ Sets up per-core caches             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      Pktgen: pktgen_packet_ctor() fills packet templates

  [2] FAST PATH - GET MBUF
      Application calls rte_mempool_get_bulk(mp, mbufs[], count)
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ DPDK Core Libraries (Mempool)          â”‚
      â”‚  Returns pre-filled mbufs from pool    â”‚
      â”‚  (uses per-core cache for performance) â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      mbuf* array returned to application

  [3] SUBMISSION TO TX QUEUE
      Application calls rte_eth_tx_burst(port, queue, mbufs[], count)
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ DPDK Core Libraries (Ethdev)           â”‚
      â”‚  p->tx_pkt_burst() dispatch            â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      PMD: mlx5_tx_burst()
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ For each mbuf:                â”‚
      â”‚  1. Write WQE descriptor      â”‚ â† WQE Ring Buffer (PMD Layer)
      â”‚  2. Store mbuf pointer        â”‚ â† SW TX Queue (elts[])
      â”‚  3. Increment wqe_pi          â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      Ring doorbell (notify NIC)

  [4] HARDWARE PROCESSING
      NIC DMA reads WQEs and packet data
              â†“
      Transmit to wire
              â†“
      Write completion to CQ

  [5] COMPLETION & FREE
      PMD: mlx5_tx_handle_completion()
              â†“
      Poll Completion Queue
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ For each completion:          â”‚
      â”‚  1. Read elts[wqe_ci]         â”‚ â† Get mbuf pointer (PMD Layer)
      â”‚  2. Call rte_mempool_put_bulk â”‚ â”€â”
      â”‚  3. Increment wqe_ci          â”‚  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
              â”‚                           â”‚
              â”‚                           â†“
              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚ DPDK Core Libraries (Mempool)      â”‚
              â”‚              â”‚  Returns mbufs to pool             â”‚
              â”‚              â”‚  (ready for next get_bulk call)    â”‚
              â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      Loop continues (mbuf reuse pattern)
```

---

## 4. TX Path Flow with Data Structures

### Complete Step-by-Step Flow

#### STEP 1: Packet Initialization (One-time setup)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Layer (Pktgen)         â”‚
â”‚                                     â”‚
â”‚  rte_mempool_create()              â”‚ â”€â”€â†’ Create mbuf mempool
â”‚            â†“                        â”‚     (via DPDK Core Libraries)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DPDK Core Libraries (Mempool)      â”‚
â”‚                                     â”‚
â”‚  â€¢ Allocate 32,768 mbufs            â”‚
â”‚  â€¢ Setup per-core caches            â”‚
â”‚  â€¢ Return mempool handle            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Layer (Pktgen)         â”‚
â”‚                                     â”‚
â”‚  pktgen_packet_ctor()              â”‚ â”€â”€â†’ Fill packet templates
â”‚  â€¢ Pre-fill headers                 â”‚     (headers, patterns)
â”‚  â€¢ Setup packet patterns            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code Location**: `pktgen.c:750+` (pktgen_packet_ctor)

**Key Point**: Mempool is **shared across cores** - potential contention point!

---

#### STEP 1.5: Fast Path - Get Pre-filled mbufs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Layer (Pktgen)         â”‚
â”‚                                     â”‚
â”‚  rte_mempool_get_bulk(mp,          â”‚ â”€â”€â†’ Request batch of mbufs
â”‚      mbufs[], count)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DPDK Core Libraries (Mempool)      â”‚
â”‚                                     â”‚
â”‚  â€¢ Check per-core cache first       â”‚
â”‚  â€¢ Return pre-filled mbufs          â”‚
â”‚  â€¢ Zero-copy (reuse pattern)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    mbufs[] ready for transmission
```

**Code Location**: `pktgen.c:1297+` (pktgen_send_pkts)

**Key API**: `rte_mempool_get_bulk()` NOT `rte_pktmbuf_alloc()`!

---

#### STEP 2: Submit to TX Queue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Layer (Pktgen)         â”‚
â”‚                                     â”‚
â”‚  tx_send_packets(mbufs[], count)   â”‚ â”€â”€â†’ Batch mbufs into array
â”‚            â†“                        â”‚
â”‚  rte_eth_tx_burst(port, queue,     â”‚ â”€â”€â†’ Call DPDK Core API
â”‚                   mbufs[], count)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DPDK Core Libraries (Ethdev)       â”‚
â”‚                                     â”‚
â”‚  p->tx_pkt_burst(...)              â”‚ â”€â”€â†’ Dispatch to PMD via
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     function pointer
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PMD Layer (MLX5)                   â”‚
â”‚                                     â”‚
â”‚  mlx5_tx_burst(mbufs[], count)     â”‚
â”‚            â†“                        â”‚
â”‚  For each mbuf:                     â”‚
â”‚    1. Build WQE descriptor         â”‚ â”€â”€â†’ Write to WQE ring at wqe_pi
â”‚    2. Store mbuf pointer           â”‚ â”€â”€â†’ Save to elts[wqe_pi]
â”‚    3. Increment wqe_pi             â”‚ â”€â”€â†’ Advance producer index
â”‚            â†“                        â”‚
â”‚  Ring doorbell (MMIO write)        â”‚ â”€â”€â†’ Notify NIC of new packets
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
        [ NIC ]
```

**Code Locations**:
- Application: `pktgen.c:463` (`tx_send_packets()`)
- DPDK Core Libraries: `dpdk/lib/ethdev/rte_ethdev.h` (inline function)
- PMD: `dpdk/drivers/net/mlx5/mlx5_tx.h` (`mlx5_tx_burst()`)

---

#### STEP 3: Hardware Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hardware (NIC)                     â”‚
â”‚                                     â”‚
â”‚  DMA reads WQEs                    â”‚ â”€â”€â†’ From WQE ring buffer
â”‚            â†“                        â”‚     (host memory)
â”‚  DMA reads packet data             â”‚ â”€â”€â†’ From mbuf data buffers
â”‚            â†“                        â”‚
â”‚  Transmit packets to wire          â”‚
â”‚            â†“                        â”‚
â”‚  Write completion entries          â”‚ â”€â”€â†’ To Completion Queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
        [ Wire ]
```

**Key Point**: All DMA operations - no CPU involvement!

---

#### STEP 4: Completion Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PMD Layer (MLX5)                   â”‚
â”‚                                     â”‚
â”‚  mlx5_tx_handle_completion()       â”‚
â”‚            â†“                        â”‚
â”‚  Poll completion queue             â”‚ â”€â”€â†’ Read CQ entries (polling!)
â”‚            â†“                        â”‚
â”‚  mlx5_tx_free_mbuf()               â”‚
â”‚            â†“                        â”‚
â”‚  For each completed packet:         â”‚
â”‚    1. Read elts[wqe_ci]            â”‚ â”€â”€â†’ Get mbuf pointer
â”‚    2. rte_mempool_put_bulk()       â”‚ â”€â”
â”‚    3. Increment wqe_ci             â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
            â”‚                            â”‚
            â”‚                            â†“
            â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚         â”‚  DPDK Core Libraries (Mempool)      â”‚
            â”‚         â”‚                                     â”‚
            â”‚         â”‚  â€¢ Return mbufs to pool             â”‚
            â”‚         â”‚  â€¢ Update per-core cache            â”‚
            â”‚         â”‚  â€¢ Ready for next get_bulk()        â”‚
            â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    mbuf reuse pattern continues
```

**Code Locations**:
- PMD: `dpdk/drivers/net/mlx5/mlx5_tx.h:542-567` (mlx5_tx_free_mbuf)
- DPDK Core Libraries: `dpdk/lib/mempool/rte_mempool.h` (rte_mempool_put_bulk)

**Key Point**: Polling-based (not interrupt-driven) for low latency!

---

## 5. Key Indices & Queue Depth Calculation

### WQE Ring Buffer Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WQE Ring Buffer (Size = 1024)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Consumer Index (wqe_ci)              Producer Index (wqe_pi)
            â†“                                    â†“
  â”â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”“
  â”ƒFreeâ”ƒFreeâ”ƒUsedâ”ƒUsedâ”ƒUsedâ”ƒUsedâ”ƒFreeâ”ƒFreeâ”ƒFreeâ”ƒFreeâ”ƒFreeâ”ƒFreeâ”ƒ
  â”—â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”»â”â”â”â”â”›
  â”‚  0    1    2    3    4    5    6    7    8    9   10   11 ...â”‚
            â†‘                    â†‘
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               Queue Depth
            (In-flight packets)
```

### Formula & Calculation

```
Queue Depth = (wqe_pi - wqe_ci) mod wqe_s
```

**Example from Logs**:
```
wqe_ci = 648    (Consumer Index - packets completed by NIC)
wqe_pi = 351    (Producer Index - packets submitted by app)
wqe_s  = 1024   (Ring size - wraps around)

Calculation (with wraparound):
  wqe_used = (351 + 1024 - 648) mod 1024 = 727 mod 1024 = 297
```

**Interpretation**:
- âœ… **297 packets in flight** (waiting for completion)
- âœ… **29% queue utilization** (297 / 1024)
- âœ… **727 free slots available**

### Queue States

| Queue Depth | Utilization | Interpretation | Action Needed |
|-------------|-------------|----------------|---------------|
| < 10% | Very Low | NIC processing faster than app submission | Can increase TX rate |
| 20-50% | **Healthy** | Balanced state | âœ… Optimal |
| 50-80% | High | Queue filling up | Monitor for drops |
| > 80% | Critical | Risk of overflow | Reduce TX rate or increase queue size |

---

## 6. Our Tracking Points

### Tracking Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Tracking Points in Multi-Layer Stack                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  ğŸ“ [APPLICATION LAYER - pktgen.c:533-565]                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  âœ… Producer Count Tracking                                       â”‚
â”‚     â€¢ What: Packets submitted by application                      â”‚
â”‚     â€¢ When: Before rte_eth_tx_burst() call                       â”‚
â”‚     â€¢ Why: Counted once per batch (not per retry)                â”‚
â”‚     â€¢ Variable: ak_txq_stats[lcore_id].producer_count            â”‚
â”‚                                                                    â”‚
â”‚  âœ… Burst Interval Tracking                                       â”‚
â”‚     â€¢ What: Cycles between consecutive bursts                     â”‚
â”‚     â€¢ When: At start of each tx_send_packets() call              â”‚
â”‚     â€¢ Why: Measure actual rate limiting effectiveness            â”‚
â”‚     â€¢ Includes: Rate limiting delay + processing overhead         â”‚
â”‚     â€¢ Variable: ak_txq_stats[lcore_id].total_burst_interval      â”‚
â”‚                                                                    â”‚
â”‚  âœ… Burst Processing Time Tracking                                â”‚
â”‚     â€¢ What: Cycles to process one burst (incl. retry loop)       â”‚
â”‚     â€¢ When: tx_send_packets() entry â†’ exit                       â”‚
â”‚     â€¢ Why: Identify bottlenecks in TX path                       â”‚
â”‚     â€¢ Variable: ak_txq_stats[lcore_id].total_burst_processing    â”‚
â”‚                                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  ğŸ“ [PMD LAYER - mlx5_tx.h:679-688]                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚  âœ… Consumer Count Tracking                                       â”‚
â”‚     â€¢ What: Packets completed by NIC                              â”‚
â”‚     â€¢ When: mlx5_tx_free_elts() (completion processing)          â”‚
â”‚     â€¢ Why: Measure actual NIC throughput                         â”‚
â”‚     â€¢ Variable: ak_txq_stats[lcore_id].consumer_count            â”‚
â”‚                                                                    â”‚
â”‚  ğŸ“ [PMD LAYER - mlx5_tx.h:3691-3715]                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚  âœ… Queue Depth Tracking                                          â”‚
â”‚     â€¢ What: WQE queue utilization (wqe_ci - wqe_pi)              â”‚
â”‚     â€¢ When: Every 10,000th completion (sampled for efficiency)   â”‚
â”‚     â€¢ Why: Monitor queue saturation                              â”‚
â”‚     â€¢ Variable: ak_txq_stats[lcore_id].total_depth               â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Metrics Collected

| Metric | Formula | Unit | Interpretation |
|--------|---------|------|----------------|
| **Producer Rate** | producer_count / duration | Mpps | App submission throughput |
| **Consumer Rate** | consumer_count / duration | Mpps | Actual NIC throughput |
| **P/C Ratio** | producer / consumer | ratio | 1.0 = balanced<br>> 1.0 = drops |
| **Avg Queue Depth** | total_depth / sample_count | packets | Queue utilization |
| **Avg Burst Interval** | total_burst_interval / burst_count | cycles | Actual burst spacing |
| **Avg Burst Processing** | total_burst_processing / burst_count | cycles | TX overhead |

---

## 7. Configuration Bug Fixed

### The Bug

**File**: `Pktgen-DPDK/app/pktgen-port-cfg.c:493`

#### âŒ BEFORE (Buggy Code)

```c
ret = rte_eth_tx_queue_setup(pid, q, pktgen.nb_rxd, pinfo->sid, &txq_conf);
                                     ^^^^^^^^^^^^^^
                                     Wrong! Using RX descriptor count
```

**Impact**:
- âŒ TX queue size = RX queue size (both 1024)
- âŒ Changing `DEFAULT_TX_DESC` had no effect
- âŒ `wqe_s` remained 1024 regardless of configuration
- âŒ Cannot independently tune TX/RX queue sizes

---

#### âœ… AFTER (Fixed Code)

```c
ret = rte_eth_tx_queue_setup(pid, q, pktgen.nb_txd, pinfo->sid, &txq_conf);
                                     ^^^^^^^^^^^^^^
                                     Correct! Using TX descriptor count
```

**Benefits**:
- âœ… TX queue size = `DEFAULT_TX_DESC` (configurable)
- âœ… Can adjust TX descriptor ring independently
- âœ… `wqe_s` reflects configured value
- âœ… Proper control over TX queue depth

---

### Configuration Flow

```
Application Layer:
  pktgen-constants.h:
    DEFAULT_TX_DESC = 2048   â† Can now configure this!
            â†“
  pktgen-main.c:462:
    pktgen.nb_txd = DEFAULT_TX_DESC
            â†“
  pktgen-port-cfg.c:493:
    rte_eth_tx_queue_setup(..., pktgen.nb_txd, ...)  â† Fixed!
            â†“
PMD Layer:
  MLX5 driver receives nb_txd and sets:
    txq->wqe_s = nb_txd
            â†“
Hardware:
  WQE ring allocated with wqe_s descriptors
```

---

## 8. Summary Table

### Layer-by-Layer Comparison

| Aspect | Application Layer | DPDK Core Libraries | PMD Layer | Hardware |
|--------|------------------|---------------------|-----------|----------|
| **ğŸ“ Code Location** | `Pktgen-DPDK/app/pktgen.c` | `dpdk/lib/mempool/`<br>`dpdk/lib/mbuf/`<br>`dpdk/lib/ethdev/` | `dpdk/drivers/net/mlx5/mlx5_tx.h` | NIC Firmware |
| **ğŸ”§ Main Functions** | `tx_send_packets()`<br>`pktgen_packet_ctor()` | Mempool:<br>â€¢ `rte_mempool_get_bulk()`<br>â€¢ `rte_mempool_put_bulk()`<br>Ethdev:<br>â€¢ `rte_eth_tx_burst()` | `mlx5_tx_burst()`<br>`mlx5_tx_handle_completion()`<br>`mlx5_tx_free_mbuf()` | DMA Engine |
| **ğŸ’¾ Data Structures** | â€¢ App RX/TX Rings<br>(optional) | Mempool:<br>â€¢ mbuf Mempool<br>â€¢ struct rte_mbuf<br>Ethdev:<br>â€¢ rte_eth_dev<br>(metadata only) | â€¢ WQE Ring<br>â€¢ SW TX Queue<br>â€¢ Completion Queue | On-chip FIFO |
| **âš™ï¸ Queue Size Config** | `nb_rxd/nb_txd`<br>âš ï¸ TX was buggy!<br>(used nb_rxd) | Mempool:<br>32,768 mbufs<br>Ethdev:<br>Pass-through | `wqe_s`<br>(based on nb_txd) | Fixed HW size |
| **ğŸ“Š Tracking Added** | â€¢ Producer count<br>â€¢ Burst interval<br>â€¢ Processing time | None | â€¢ Consumer count<br>â€¢ Queue depth | None |
| **ğŸ“ Typical Size** | 1024 descriptors<br>(nb_rxd/nb_txd) | 32K mbufs<br>(mempool) | 1024 descriptors<br>(wqe_s) | HW-dependent |
| **ğŸ¯ Responsibility** | Packet generation<br>Rate limiting<br>Retry logic | Mempool:<br>Memory pool mgmt<br>Per-core caching<br>Ethdev:<br>API abstraction<br>PMD dispatch | Hardware interface<br>Descriptor mgmt<br>Completion handling<br>mbuf lifecycle | DMA & TX |
| **ğŸ”„ APIs Used** | Calls:<br>â€¢ `rte_mempool_get_bulk()`<br>â€¢ `rte_eth_tx_burst()` | Provides:<br>â€¢ Mempool APIs<br>â€¢ Ethdev APIs<br>Used by:<br>â€¢ Application<br>â€¢ PMD | Calls:<br>â€¢ `rte_mempool_put_bulk()`<br>(in completion path) | Hardware ops<br>(DMA, TX) |

---

## Key Takeaways

### 1ï¸âƒ£ **Layered Architecture**
- Four distinct layers: Application â†’ DPDK Core Libraries â†’ PMD â†’ Hardware
- **DPDK Core Libraries** is an independent infrastructure layer containing:
  - Mempool Library (`dpdk/lib/mempool/`)
  - Mbuf Library (`dpdk/lib/mbuf/`)
  - Ethdev Library (`dpdk/lib/ethdev/`)
- Clean separation with well-defined APIs
- Function pointers enable driver abstraction (Ethdev â†’ PMD dispatch)

### 2ï¸âƒ£ **Mempool is NOT Part of Application or Ethdev**
- âœ… **Mempool is a separate DPDK Core Library**
- Located at `dpdk/lib/mempool/` (peer to `ethdev/` and `mbuf/`)
- Used by **both Application AND PMD layers**:
  - Application: `rte_mempool_get_bulk()` to get mbufs
  - PMD: `rte_mempool_put_bulk()` to return mbufs after completion
- Shared resource with per-core caching for performance

#### ğŸ“Š Mempool Internal Architecture: Per-Core Cache + Global Ring

**âš ï¸ Common Misconception**: Mempool does NOT use simple producer/consumer indices for all operations!

**Two-Tier Structure** (`dpdk/lib/mempool/rte_mempool.h`):

**1. Per-Core Cache (Fast Path - Stack Structure)**
- **Structure**: `cache->objs[]` array + `cache->len` (stack height)
- **NOT** a ring with producer/consumer indices!
- **Location**: `rte_mempool.h:1530` (get), `1404` (put)

**Get Operation** (`rte_mempool_do_generic_get`, line 1530-1559):
```c
// "The cache is a stack" (line 1530 comment)
cache_objs = &cache->objs[cache->len];  // Stack top
cache->len -= n;                        // Decrement height
for (index = 0; index < n; index++)
    *obj_table++ = *--cache_objs;       // Pop from stack
```

**Put Operation** (`rte_mempool_do_generic_put`, line 1404-1421):
```c
cache_objs = &cache->objs[cache->len];  // Stack top
cache->len += n;                        // Increment height
rte_memcpy(cache_objs, obj_table, ...); // Push to stack
```

**Key Insight**: Uses **stack semantics** (len only), NOT ring indices (producer/consumer)

**2. Global Ring (Slow Path - Ring Structure)**
- Used ONLY when per-core cache miss or flush
- **Location**: `rte_mempool.h:1413` (enqueue), `1570` (dequeue)
- This layer DOES use producer/consumer ring (`rte_mempool_ops_enqueue_bulk`, `rte_mempool_ops_dequeue_bulk`)

**Cache Refill** (line 1570-1582):
```c
driver_dequeue:
    ret = rte_mempool_ops_dequeue_bulk(mp, obj_table, remaining);
    // â†‘ Global ring dequeue (producer/consumer ring here)
```

**Cache Flush** (line 1413):
```c
rte_mempool_ops_enqueue_bulk(mp, cache_objs, cache->len);
// â†‘ Flush cache to global ring (producer/consumer ring)
```

**Performance Optimization**:
- **Fast path**: Per-core cache (no atomic ops, just len increment/decrement)
- **Slow path**: Global ring access (atomic producer/consumer operations)
- Cache hit rate typically >95% â†’ most operations avoid global ring

**Both Application AND PMD use same API**:
- `rte_mempool_get_bulk()` - Application gets mbufs
- `rte_mempool_put_bulk()` - PMD returns completed mbufs
- Both go through: per-core cache (stack) â†’ global ring (fallback)
- No difference in mechanism, only usage context differs

### 3ï¸âƒ£ **Multiple Queue Concepts and Physical Locations**

**âš ï¸ Terminology Clarification:**
- **SQ (Send Queue)** = **WQE Ring** = descriptor ring buffer (DMA-mapped memory)
- **TX Control Structure** = `struct mlx5_txq_data` = manages WQE Ring (NOT a queue!)
  - Contains: wqe_ci/wqe_pi (WQE Ring indices), wqes (â†’ WQE Ring), elts[] (mbuf pointers)
  - elts[] and WQE Ring: **same size (1024)** but **NOT 1:1 mapping** (see N:M mapping below)
- **Why confusing naming?** "wqe_pi/wqe_ci" are indices **for the WQE Ring**, not for mlx5_txq_data itself

#### âš ï¸ CRITICAL: WQE Ring â†” elts[] N:M Mapping (NOT 1:1!)

**Common Misconception**: "elts[i] always corresponds to WQE[i]" âŒ

**Reality**: Mapping depends on configuration (`mlx5_tx.h:3718-3724`)

**Scenario 1: No Data Inlining** (Multiple packets â†’ 1 WQE)
```
elts[]:      mbuf0*  mbuf1*  mbuf2*  mbuf3*    â† 4 mbuf pointers
               â†“       â†“       â†“       â†“
WQE Ring:    [    WQE 0 (4 packet ptrs)    ]   â† 1 descriptor
```
- **Ratio**: 1 WQE : up to 4 elts[]
- **Bottleneck**: **elts[] becomes scarce first** (consumed 4x faster)
- **Example**: WQE_free=100, elts_free=5 â†’ can only send 5 packets (not 100!)

**Scenario 2: Data Inlining Enabled** (1 packet â†’ Multiple WQEs)
```
elts[]:      mbuf0*                          â† 1 mbuf pointer
               â†“ (data copied to WQEs)
            [packet data]
               â†“
WQE Ring:    [ WQE 0 - inline data part 1 ]
             [ WQE 1 - inline data part 2 ]  â† Multiple descriptors
             [ WQE 2 - inline data part 3 ]
```
- **Ratio**: 1 elts[] : multiple WQEs (large packet needs multiple descriptors)
- **Bottleneck**: **WQE Ring becomes scarce first**
- **Example**: elts_free=100, WQE_free=2 â†’ might not send even 1 large packet!
- **Key**: Inline copies data to WQE, but **still stores mbuf* in elts[]** (mlx5_tx.h:2483)
  - Why? Must free mbuf after TX completion (return to mempool)
  - Inline = NIC doesn't DMA from mbuf, but mbuf memory still needs cleanup

**Why Both Checks Are Required** (`mlx5_tx.h:3732`):
```c
if (unlikely(!loc.elts_free || !loc.wqe_free))
    goto burst_exit;  // Partial send!
```
- Different modes have **different bottlenecks**
- Configuration-dependent: `MLX5_TXOFF_CONFIG(INLINE)`
- Packet size varies: runtime behavior unpredictable
- Both resources consumed **independently**

**Partial Send Conditions**:
- **elts[] full**: Can't store more mbuf pointers (no inline or small inline)
- **WQE Ring full**: Can't write more descriptors (large packet inline)

#### ğŸš€ Why Inline Mode is a Performance Win (Not Waste!)

**Common Misunderstanding**: "Inline copies data AND keeps original mbuf = wasteful duplication" âŒ

**Reality**: Inline is a **memory efficiency optimization** through **early free** âœ…

**No Inline Mode Flow** (Traditional DMA):
```
1. Application: Allocate mbuf + write data
2. TX burst: Store mbuf DMA address in WQE
3. NIC: DMA read from mbuf memory (~200ns PCIe latency)
4. Wait for TX completion... (1-10Î¼s)
5. Completion handler: Finally free mbuf to mempool

Problem: mbuf locked until completion (long time!)
```

**Inline Mode Flow** (Copy + Early Free):
```
1. Application: Allocate mbuf + write data
2. TX burst:
   a) Copy data to WQE (mlx5_tx.h:1187):
      rte_memcpy(wqe, mbuf->data, len);  // ~10ns from CPU cache

   b) Immediately free mbuf (mlx5_tx.h:3416):
      rte_pktmbuf_free_seg(loc->mbuf);   // â† Key optimization!

3. NIC: Read directly from WQE (no DMA to mbuf needed)
4. Completion: No mbuf cleanup needed (already freed!)

Advantage: mbuf returned to mempool immediately!
```

**Code Evidence** (`mlx5_tx.h:3406-3416`):
```c
// Copy packet data into WQE inline
mlx5_tx_eseg_data(txq, loc, wqe, vlan, inlen, 0, olx);

/*
 * Packet data are completely inlined,
 * free the packet immediately.
 */
rte_pktmbuf_free_seg(loc->mbuf);  // â† Early free!
```

**Inline Copy Implementation** (`mlx5_tx.h:1148-1187`):
```c
mlx5_tx_eseg_data(...) {
    // Get source from mbuf
    psrc = rte_pktmbuf_mtod(loc->mbuf, uint8_t *);  // line 1148

    // Destination in WQE
    pdst = (uint8_t *)(es + 1);  // line 1152

    // Fast copy (optimized for small packets)
    rte_mov16(pdst, psrc);       // line 1169 - 16 bytes
    rte_memcpy(pdst, psrc, part); // line 1187 - remaining data
}
```

**Performance Analysis**:

| Metric | No Inline (DMA) | Inline (Copy+Free) | Winner |
|--------|-----------------|-------------------|---------|
| **Small packet (64B)** |
| Data transfer | DMA setup + PCIe (~300ns) | CPU copy from cache (~10ns) | Inline âœ… |
| Mbuf lifetime | Until completion (~1-10Î¼s) | Immediate free (~100ns) | Inline âœ… |
| Memory pressure | High (many in-flight mbufs) | Low (quick recycle) | Inline âœ… |
| **Large packet (1500B)** |
| Data transfer | DMA (~500ns) | CPU copy (~150ns) | Inline âœ… |
| WQE consumption | 1-2 descriptors | 4-6 descriptors | No-inline âœ… |
| Memory pressure | Moderate | Very low | Inline âœ… |

**Why "Copy then Free" is Smart**:

1. **Memory Recycling Speed** âš¡
   - No inline: mbuf tied up for microseconds (waiting for completion)
   - Inline: mbuf freed in nanoseconds â†’ instant reuse
   - High packet rate â†’ faster mempool turnover = fewer mbufs needed

2. **Cache Efficiency** ğŸ¯
   - Copy from mbuf (hot in CPU cache) â†’ WQE (also in cache)
   - Much faster than DMA setup + PCIe round-trip
   - For 64B packets: copy ~10ns vs DMA ~300ns

3. **Simplified Memory Model** ğŸ§©
   - mbuf always complete object (never "moved" or corrupted)
   - Clean abstraction: PMD doesn't modify application memory
   - Easier debugging: no partial/zombie mbufs

4. **PCIe Bandwidth Savings** ğŸ’¾
   - No inline: 2x PCIe traffic (descriptor + DMA data read)
   - Inline: 1x PCIe traffic (descriptor with embedded data)
   - Important for high packet rates on shared PCIe bus

**Trade-off Sweet Spot**:
```
Inline wins:   Packet size < 256B  (copy cheaper than DMA)
No-inline wins: Packet size > 1KB   (DMA cheaper than copy + WQE bloat)
Mixed traffic:  Configure threshold (txq->inlen_send)
```

**Why Not "Move" Instead of "Copy"?**

Hypothetical (bad) approach:
```c
// Don't do this!
memcpy(wqe, mbuf->data, len);
mbuf->data = NULL;        // Corrupt mbuf structure
mbuf->data_len = 0;       // Complex state management
// Now what? Partial mbuf? Special handling everywhere?
```

Current approach (clean):
```c
// Simple and correct:
memcpy(wqe, mbuf->data, len);        // Copy data
rte_pktmbuf_free_seg(mbuf);          // Complete object free
// mbuf returned intact to mempool, ready for reuse
```

**Key Insight**: The "duplication" lasts only ~100ns (copy time), then original is freed. Not wastefulâ€”it's **temporal efficiency through spatial duplication**.

**âš ï¸ WQE Structure and Why elts[] is Needed:**
- **WQE = Hardware Descriptor = Work Instruction for NIC**
  - **NOT** "describing the hardware"
  - **YES** "describing how hardware should process data"
  - Tells NIC: "Read from this address, send this many bytes"
- **Each WQE contains**:
  - `pbuf`: DMA physical address (where to read, NOT mbuf pointer!)
  - `bcount`: Byte count to transmit (how many bytes)
  - `lkey`: Memory protection key
- **WQE does NOT contain mbuf pointer** because:
  - NIC hardware only understands DMA addresses (physical memory)
  - NIC cannot use virtual pointers (like `mbuf*`)
  - WQE is instruction, not the data itself
- **elts[] array tracks mbuf pointers in parallel**:
  - When WQE[i] is in-flight â†’ elts[i] stores corresponding mbuf*
  - On completion â†’ driver retrieves elts[i] to return mbuf to mempool
  - This is why elts[] is essential: it's the only place tracking which mbuf belongs to which WQE

**ğŸ“ Size Determination (both from same DEFAULT_TX_DESC parameter):**
- **Application**: Sets `DEFAULT_TX_DESC` (e.g., 1024) in config
- **PMD allocation** (`mlx5_txq.c:1135, 1168, 708`):
  - `elts[]` size = `desc` (exact value)
  - WQE Ring size = `1 << log2(desc)` (rounded to power of 2)
- **Key insight**: If `desc` is not power of 2, it's rounded up
  - Example: `desc=1000` â†’ both become 1024 (2^10)
  - Example: `desc=2048` â†’ both stay 2048 (2^11)
- **Result**: Both arrays always same size, guaranteeing 1:1 parallel mapping

**Physical Locations:**
- **Application**: Temporary working buffers (pointer arrays, ~32 entries)
  - Location: Host memory (application space)
- **DPDK Core Libraries**: mbuf mempool (packet buffer pool)
  - Location: Host memory (DMA-mapped)
- **PMD** (all in host memory!):
  - **WQE Ring/SQ**: Descriptor ring buffer
    - Location: **HOST MEMORY** (DMA-mapped, NOT on NIC chip!)
    - Evidence: `drivers/common/mlx5/mlx5_common_devx.c:118-127`
    - Allocated by PMD using `mlx5_malloc_numa_tolerant()`
    - Registered for DMA access with `mlx5_os_umem_reg()`
    - NIC reads descriptors from here via DMA
  - **Completion Queue (CQ)**: Completion status buffer
    - Location: Host memory (DMA-mapped)
    - NIC writes completion entries here via DMA
  - **TX Control Structure (mlx5_txq_data)**: Metadata/management
    - Location: Host memory (regular)
    - Driver uses this to manage WQE Ring (not accessed by NIC)
    - Contains parallel elts[] array tracking mbuf pointers
- **Hardware**: On-chip FIFO (internal NIC buffer)
  - Location: NIC chip (not accessible by software)
- **Key Insight**: Only the on-chip FIFO is actually on NIC hardware
  - WQE Ring, CQ, elts[] are all in **host RAM**
  - NIC accesses WQE Ring/CQ via DMA, not elts[]

### 4ï¸âƒ£ **Pktgen's Zero-Copy mbuf Reuse Pattern**
- **NOT using** `rte_pktmbuf_alloc()` / `rte_pktmbuf_free()` in fast path
- **Initialization**: Create packet templates once with `pktgen_packet_ctor()`
- **Fast path**: Use `rte_mempool_get_bulk()` to retrieve pre-filled mbufs
- **Completion**: PMD calls `rte_mempool_put_bulk()` to return mbufs
- Zero-copy optimization - no per-packet allocation overhead

### 5ï¸âƒ£ **Performance Tracking**
- Producer/Consumer tracking at different layers reveals bottlenecks
- Queue depth monitoring prevents overflow
- Burst timing measurements identify latency sources

### 6ï¸âƒ£ **Configuration Bug Fixed**
- TX/RX queue sizes must be independently configurable
- Bug: `pktgen-port-cfg.c:493` used `nb_rxd` instead of `nb_txd`
- Fix enables proper tuning of TX descriptor ring sizes
- Hardware queue size (`wqe_s`) now correctly derives from `nb_txd`

### 7ï¸âƒ£ **Complete Data Flow**
```
[Initialization]
rte_mempool_create() â†’ pktgen_packet_ctor() (fill templates)
     â†“
[Fast Path TX]
rte_mempool_get_bulk() â†’ rte_eth_tx_burst() â†’ mlx5_tx_burst() â†’
WQE write â†’ DMA read â†’ wire TX
     â†“
[Completion]
Poll CQ â†’ mlx5_tx_free_mbuf() â†’ rte_mempool_put_bulk() â†’ mbuf reuse
```

Each step involves different data structures and layers!

---

## Additional Notes

### Multi-segment mbuf (Jumbo Frames)

For packets larger than 2048 bytes (e.g., Jumbo Frames), DPDK uses **chained mbufs**:

```
Example: 9000-byte Jumbo Frame

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1st mbuf (head) â”‚ pkt_len=9000, data_len=2048, nb_segs=5
â”‚ [2048 bytes]    â”‚ next â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2nd mbuf        â”‚ data_len=2048
â”‚ [2048 bytes]    â”‚ next â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3rd mbuf        â”‚ data_len=2048
â”‚ [2048 bytes]    â”‚ next â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4th mbuf        â”‚ data_len=2048
â”‚ [2048 bytes]    â”‚ next â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5th mbuf (tail) â”‚ data_len=808
â”‚ [808 bytes]     â”‚ next = NULL
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key mbuf fields for multi-segment support:**
- `pkt_len`: Total packet length (all segments)
- `data_len`: Data length in this mbuf
- `nb_segs`: Number of segments in this packet
- `next`: Pointer to next mbuf in chain

**Note**: Most benchmarks use standard 64-byte packets, so multi-segment handling is rare in typical test scenarios.

---

## References

- **DPDK Documentation**: https://doc.dpdk.org/guides/prog_guide/
- **MLX5 PMD Guide**: https://doc.dpdk.org/guides/nics/mlx5.html
- **Pktgen Documentation**: https://pktgen-dpdk.readthedocs.io/

---

**Last Updated**: 2025-01-05
**Project**: DPDK Performance Benchmarking & Optimization
**Repository**: `/homes/inho/Autokernel/dpdk-bench/`
